<h1 align="center">ğŸš€ JAX / XLA Whole-Program Optimizer</h1><p align="center">Compiler-Level ML Optimization with JIT Fusion</p><p align="center"><img src="https://img.shields.io/badge/JAX-0.7.2-9cf?style=for-the-badge&logo=google"><img src="https://img.shields.io/badge/XLA-Compiler-blue?style=for-the-badge"><img src="https://img.shields.io/badge/Paradigm-Functional-success?style=for-the-badge"></p>ğŸ“Œ OverviewTraditional deep learning frameworks execute operations one-by-one, which introduces significant Python overhead. This project utilizes JAX and the XLA (Accelerated Linear Algebra) compiler to fuse an entire training step into a single optimized machine-code executable.ğŸ§  Core Engineering PrinciplesFunctional Programming: Parameters are handled as immutable data structures, allowing for pure function transformations like jax.grad and jax.jit.XLA Compilation: Instead of interpreting Python code, XLA traces the step function and creates an optimized graph where operations are fused at the hardware level.ğŸ“Š Benchmarking ResultsBy using @jax.jit, the entire forward pass, loss calculation, and gradient update are optimized into a single GPU call.ModeStep Latency (ms)JIT-Compiled Training Step0.204 msMeasured on NVIDIA T4 GPU with a 128x128 input dimension.ğŸ—ï¸ Project Structurecore/model.py: Functional definition of a linear model.core/trainer.py: JIT-compiled value_and_grad training logic.benchmark.py: Performance measurement script with JAX warmup.ğŸš€ Getting StartedBashpip install -r requirements.txt
python benchmark.py
